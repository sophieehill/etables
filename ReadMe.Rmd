---
title: "Errors and inconsistencies in quantitative results reported in Gonçalves et al. (2025)"
output: 
  rmarkdown::github_document:
    toc: true
    number_sections: false
date: "2025-09-06"
author: "Sophie E. Hill"
---

```{css echo=FALSE, include = FALSE}
.bordered {
  display: inline-block;
}

.bordered img {
  border: 1px solid black;
  display: block;
}
```

```{r setup, include=FALSE, warning=FALSE}

knitr::opts_chunk$set(warning = FALSE)

library(tidyverse)
options(scipen=10000)

# Pre-formatting:
# removed estimates that are missing due to being "reference categories"
# saved values as characters to preserve formatting
# saved original p values (p_original) and then a version without non-numeric characters like "<0.001" (p_char)
# Replaced "Trail-making test", "Trail-Making test" with "Executive function" to be consistent across tables
# n's are taken from the table title, except for the subsets which I added:
# Tables 1, 2, 3, 5; n = 12,772
# Table 4; n = 6,041
# Table 6, 7; Subset <60 n=10,103; subset 60+ n=2,669
# Table 8, 9; Without diabetes (n = 11,363) With diabetes (1,409)

# Read all variables as characters to preserve formatting
dat <- read.csv("data/appendix_tables.csv", colClasses = "character")
glimpse(dat)

dat <-
  dat |>
  mutate(
    table_id = as.integer(table_id),
    n = as.integer(n),
    est = as.numeric(est),
    ci_lower = as.numeric(ci_lower),
    ci_upper = as.numeric(ci_upper),
    p = as.numeric(p),
    outcome = str_replace_all(outcome, "Trail-Making test", "Executive function"),
    subset = case_when(
      subset=="< 60 years" ~ "<60",
      subset=="60+ years" ~ "60+",
      subset=="Without diabetes" ~ "w/o db",
      subset=="With diabetes" ~ "w db",
      n==12772 ~ "All",
      n==6041 ~ "Complete",
      TRUE ~ subset
      ),
    model = case_when(
      model=="Unadjusted" ~ "Unadj",
      model=="Model 1" ~ "Mod 1",
      model=="Model 2" ~ "Mod 2",
      is.na(model) ~ "Mod 2", # Tables 5-9 only show Mod 2 estimates
      TRUE ~ model
    ),
    var = str_replace_all(var, "Tertile ", "T"),
    outcome = case_when(
      outcome=="Verbal fluency" ~ "VF",
      outcome=="Global cognition" ~ "GC",
      outcome=="Executive function" ~ "EF",
      outcome=="Memory" ~ "M",
      TRUE ~ outcome
    )
  ) |>
  select(X, table_id, 
         n, model, subset, var, outcome,
         est, ci_lower, ci_upper, p, 
         est_char, ci_lower_char, ci_upper_char, p_char, p_original)


# _char = variable stored as character (to preserve formatting from table)
# no suffix = variable stored as numeric
# p_original = original values, including non-numeric characters (e.g. "<0.001")

# Create "long" version of data
# where est, ci_lower, ci_upper are turned into "value"
# so we can easily count how many are multiples of 0.008
dat_8 <- 
  dat |>
  select(table_id, n, var, outcome, 
         subset, model, p,
         est, ci_lower, ci_upper) |>
  pivot_longer(cols = c("est", "ci_lower", "ci_upper")) |>
  mutate(div8 = ifelse(value %% 0.008 == 0, 1, 0)) 

# Import estimates from text of paper
# Note: only estimate and CIs are extracted, not the variables/model specs
# Search in the paper to match
pap <- read.csv("data/paper_text_estimates.csv", colClasses = "character")

pap <- 
  pap |>
  rename(est = estimate) |>
  mutate(
    est_char = est,
    ci_lower_char = ci_lower,
    ci_upper_char = ci_upper,
    est = as.numeric(est),
    ci_lower = as.numeric(ci_lower),
    ci_upper = as.numeric(ci_upper)
  )

```


This post identifies a number of errors and inconsistencies in the quantitative results presented in [this paper](https://www.neurology.org/doi/10.1212/WNL.0000000000214023), which claims to find a relationship between consumption of low- and no-calorie sweeteners (LNCS) and cognitive decline:

> Gonçalves, Natalia Gomes, Euridice Martinez-Steele, Paulo A. Lotufo, et al. ‘Association Between Consumption of Low- and No-Calorie Artificial Sweeteners and Cognitive Decline’. Neurology 105, no. 7 (2025): e214023. <https://doi.org/10.1212/WNL.0000000000214023>.

<br>

![](images/paper_title.png){width="100%"}

<br>

## Example

For example, consider this estimate, reported in the abstract on page 1:

![](images/abstract.png){width="500"}


> "Among participants aged younger than 60 years, consumption of combined LNCSs in the highest tertiles was associated with a faster decline in verbal fluency (second tertile: **β = −0.016, 95% CI −0.040 to −0.008** ..."

It is immediately obvious that the confidence interval $(-0.040, -0.008)$ is not centred around the point estimate $(-0.016)$. 

``` {r}
(-0.040 + -0.008)/2
```
Perhaps it is a typo. Let's look up this value in appendix eTable 6:

<br>

![](images/table6_p.png){width="500"}

<br>

:white_check_mark: The values for the estimate and confidence interval match. 

_But_ now we can see another issue... 

The 95% confidence interval does not cover 0, so the p-value must be $<0.05$, by definition. 

However, the value reported here is $p = 0.234$.

That is such a large discrepancy that it cannot be explained as a rounding error.

In fact, there is a more obvious, though concerning, explanation.

If we change the upper bound of the confidence interval from $-0.008$ to $+0.008$, then the estimate is now correctly centred:

```{r}
(-0.040 + 0.008)/2
```

Since the confidence interval takes the form $CI = Estimate \pm (1.96 \times SE)$, we can back out the standard error like this:

``` {r}
ci_lower <- -0.040
ci_upper <- 0.008
se <- (ci_upper - ci_lower)/(2*1.96)
se
```
With this value, we can calculate the p-value directly:

```{r}
est <- -0.016
z <- abs(est / se)
p <- 2 * (1 - pnorm(z))
p
```
This gives us $p = 0.191$, much closer to the value reported in the table of $p = 0.234$. 

(We would not expect the values to match exactly since the confidence intervals are only reported to 3 decimal places and this will introduce rounding errors in the computation of the p-value.)

_In other words: one of the key estimates presented in this paper likely contains a typo in its confidence interval. Rather than being evidence of an association between sweeteners and cognitive decline, as claimed in the text, it is not statistically significant._

## Summary

Unfortunately, this example is not an isolated issue.

Across 9 appendix tables with 256 regression coefficients, I find:

- **5** cases of **estimates outside their reported confidence interval**
- **46 duplicates** (i.e. estimate/CI combinations with an identical match in the same table)
- At least **20 cases** of **asymmetric confidence intervals**, which cannot be explained as rounding errors
- **17 values rounded to 4 decimal places** (all others to 3 decimal places), indicating manual editing of tables
- Over **50%** of all estimates and confidence intervals are **multiples of 0.008**, indicating possible post-hoc rounding and rescaling not mentioned in the text



It is difficult to diagnose exactly what is going on without access to the underlying data and code. 

However, two broad conclusions can be drawn:

1. There are many **errors and inconsistencies** in the results which appear to be the result of manual editing.
2. There is **not enough information** in the paper and appendix about how the results were generated.


## Context

The replication materials, including the underlying data and code for their analyses, are not available. So my comments are based only on the results presented in the paper and the appendix.

In order to examine the results systematically, I extracted two datasets:

1. CSV of all estimates cited in the main text (64 estimates): `paper_text_estimates.csv`
2. CSV of all regression tables in the appendix (9 tables, 256 estimates): `appendix_tables.csv`

Both datasets are available in this repo. (Note that both were created with LLM-aided extraction, so there may be errors. Always check values against the paper or appendix pdf.)

The first dataset contains estimates and confidence intervals cited in the main text of the paper in this format $(\beta = −0.016$, 95% CI $−0.040$ to $−0.008)$, along with the page number.

The second dataset contains estimates, confidence intervals, and p-values from all 9 tables in the appendix (labelled "eTable 1", "eTable 2", etc.).

The tables report results from different specifications of the core model, where the explanatory variable is consumption of sweeteners interacted with a time variable and the outcome is a measure of cognitive health, on various subsets of the data.

**Explanatory variables:**

- Categorical variable for tertiles of total LNCS consumption in mg
- Binary variable for daily LNCS consumption (vs no/sporadic consumption)
- Continuous variable of individual LNCS consumption in mg

**Outcome variables:**

- Memory (z-score of memory tests)
- Verbal fluency (z-score average of phonemic and semantic verbal fluency tests)
- Executive function (z-score of Trail-Making Test B)
- Global Cogition (composite score)

**Subsets:**

- Full sample (n = 12,772)
- Complete cases (n = 6,041)
- Under/over 60s (n = 10,103 / n = 2,669)
- Without/with diabetes (n = 11,363 / n = 1,409)

**Models:**

- Unadjusted
- Model 1 (including age, sex, race/ ethnicity, education, and income)
- Model 2 (including Model 1 + physical activity, body mass index (cubic function), hypertension, diabetes, cardiovascular disease, depressive symptoms, alcohol consumption, smoking, total calories, and MIND diet)




## 1. Estimates outside confidence interval

I have identified **5 cases** in the appendix tables (9 tables, 256 estimates) where the estimates lies outside its reported confidence interval:

```{r echo=TRUE}
dat |> 
  filter(est < ci_lower | est > ci_upper) |>
  select(table_id, model, outcome, subset, var, 
         est, ci_lower, ci_upper)

```

The estimates in rows 2-5 all come from eTable 9 and likely represent sign errors. This is clearest for the estimates from rows 2 and 4 since they have larger values:

-   $0.035$ $(0.104, 0.176)$ $\rightarrow$ the lower bound should be $-0.104$
-   $0.098$ $(-0.222, 0.026)$ $\rightarrow$ the estimate should be $-0.098$

```{r eval=TRUE, echo=TRUE}
(-0.104 + 0.176)/2
(-0.222 + 0.026)/2
```

Rows 3 and 5 are probably also sign errors, though it's harder to tell with these values: 

- $0.001$ $(-0.002, 0.000)$
- $0.001$ $(-0.001, 0.000)$


![](images/1_table9.png){width="500"}


<br>
<br>

Row 1 is harder to diagnose. This is the coefficient for Aspartame on Memory among the <60s in eTable 7: 

$-0.016$ $(-0.001, 0.001)$

All the coefficients for Aspartame on the other outcome variables in this table are 0.000.

![](images/1_table7.png){width="500"}

<br>
<br>

There is also **1 case** where an estimate cited in the paper lies outside its confidence interval (p.6):

> "Figure 3 shows the association between individual LNCS consumption and cognitive decline. There was a faster rate of decline in memory, verbal fluency, and global cognitive with higher consumption of ... sorbitol (memory: β = −0.001, 95% CI −0.001 to −0.0001; verbal fluency: β = −0.0008, 95% CI −0.001 to −0.0003; global cognition: **β = −0.0006, 95% CI −0.001 to −0.002**), ... (Figure 3 and eTable 5)"

Clearly this is a typo, since the confidence interval as written is backwards: $(−0.001$, $−0.002)$
Checking against eTable 5 in the appendix, we can see that the CI upper bound was mistranscribed in the paper. 

The correct values are: $-0.0006$ $(-0.001; -0.0002)$

``` {r include = FALSE}
pap |>
  filter(est < ci_lower | est > ci_upper)

```

**Mini summary**: :triangular_flag_on_post: 6 problematic cases identified (of which: 5 likely typos, 1 unexplained)



## 2. Duplicate values

Many of the tables contain identical estimates and CIs. 

Defining duplicates as rows where `(estimate, ci_lower, ci_upper)` has an identical match within the same table, there are **46 duplicates**. 

6 out of the 9 tables contain duplicate values (eTables 1, 2, 4, 6, 7, 9).

eTable 7 has a particularly large proportion of duplicates ($\frac{19}{56} = 34\%$ of all estimates in the table).


```{r echo = FALSE}

check_dupes_within_table <- function(df, cols, table_col = "table_id") {
  df %>%
    group_by(!!sym(table_col)) %>%
    mutate(row_num = row_number()) %>%
    group_by(!!!syms(c(table_col, cols))) %>%
    filter(n() > 1) %>%
    arrange(!!sym(table_col), !!!syms(cols)) %>%
    ungroup()
}


dupes <- check_dupes_within_table(dat, c("est", "ci_lower", "ci_upper")) |>
  select(table_id, model, outcome, subset, var, est, ci_lower, ci_upper, p) 

cat("Number of duplicated est/CIs, by table:")
dupes |> group_by(table_id) |> tally() |> as.data.frame()

cat("Duplicated est/CIs:")
dupes |> rename(id = table_id) |> as.data.frame()

```

It is useful to highlight the duplicates on the original tables to see where they occur. 

<br>

![](images/dupe_1.png){width="500"}

<br> 
<br>

![](images/dupe_2.png){width="500"}


<br>
<br>

![](images/dupe_4.png){width="500"}

<br>
<br>


![](images/dupe_6.png){width="500"}

<br>
<br>


![](images/dupe_7.png){width="500"}

<br>
<br>


![](images/dupe_9.png){width="500"}

<br>


## 3. Asymmetric confidence intervals 

While confidence intervals can be asymmetric in some contexts (e.g. bootstrapping, log-transformations), there is no indication in the paper that those contexts are relevant here. The tables present results from "linear mixed-effects models", with estimates and 95% confidence intervals. 

So we expect the confidence intervals here to be symmetric around their point estimates. 

However, even accounting for rounding errors, we can find many examples of asymmetric confidence intervals. 

We have already found one obvious example in the abstract of the paper.

![](images/abstract.png){width="500"}

But it is easy to find more. 

For example, we can filter the data to find estimates that do not match the middle of the confidence interval but have the same absolute value. These are clearly more sign errors:


```{r echo = FALSE}

dat <- dat |>
  mutate(
    ci_mid = round((ci_lower + ci_upper)/2, 4),
    est_ci_mid_diff = abs(est - ci_mid)
    )

dat |>
  filter(!(est == ci_mid)) |>
  filter(abs(est) == abs(ci_mid)) |>
  select(table_id, model, outcome, subset, var, est, ci_mid, ci_lower, ci_upper) |>
  as.data.frame()


```

Note: the estimate for Tagatose on Verbal fluency in Table 9 was already identified as problematic in Section 1, since here the sign error means that the estimate $(0.098)$ is outside its confidence interval $(-0.222, 0.026)$. 

However, we have now found two more problematic cases. These estimates are within their confidence intervals but have the wrong sign. 


Next, let's look for more cases like the one in the abstract, where the confidence interval _would_ be symmetric if one of the bounds changed sign. 

To do this, let's filter the data to cases where the estimate has the same absolute value as the middle of the amended confidence interval (where one bound has flipped sign).

``` {r echo = TRUE}
dat |>
  mutate(
    ci_mid2 = (ci_lower - ci_upper)/2
    ) |>
  filter(!(est == ci_mid)) |>
  filter(abs(est) == abs(ci_mid2)) |>
  select(table_id, model, outcome, subset, var, est, ci_mid, ci_mid2, ci_lower, ci_upper) |>
  as.data.frame()

```
We have found two more examples in addition to the estimate of $-0.016$ from the abstract.

- Original: $-0.005$ $(-0.008, 0.002)$
- Amended: $-0.005$ $(-0.008, -0.002)$

```{r}
(-0.008 + -0.002)/2
```
- Original: $0.001$ $(-0.002, 0.000)$
- Amended: $0.001$ $(0.002, 0.000)$

```{r}
(0.002 + 0.000)/2
```


How many other estimates have asymmetric confidence intervals? It depends on how much tolerance we allow for rounding errors. 


``` {r echo = FALSE}
cat("Number of estimates more than 0 from CI mid: ", sum(dat$est_ci_mid_diff>0))
cat("Number of estimates more than 0.001 from CI mid: ", sum(dat$est_ci_mid_diff>0.001))
cat("Number of estimates more than 0.002 from CI mid: ", sum(dat$est_ci_mid_diff>0.002))
cat("Number of estimates more than 0.003 from CI mid: ", sum(dat$est_ci_mid_diff>0.003))
cat("Number of estimates more than 0.004 from CI mid: ", sum(dat$est_ci_mid_diff>0.004))

```


Recall that almost all values in the tables are presented to 3 decimal places. 

Let's focus on examples that are unlikely to be attributable to rounding errors. Here are 20 cases where the difference between the estimate and the middle of the confidence interval is greater than 0.004: 


``` {r echo = FALSE}
cat("Cases where abs(est - ci_mid) > 0.004:")
dat |>
  filter(est_ci_mid_diff>0.004) |>
  arrange(-est_ci_mid_diff) |>
  select(table_id, model, outcome, subset, var, est, 
         ci_mid, est_ci_mid_diff, ci_lower, ci_upper) |>
  rename(diff = est_ci_mid_diff) |>
  rename(id = table_id) |>
  as.data.frame()
```


## 4. Inconsistent rounding

Numerical values presented in the text of the paper and in the appendix tables are rounded inconsistently. The majority of the values are given to 3 decimal places, but a small number are given to 4 instead.

```{r echo=FALSE}

# Count decimal places

count_dp <- function(x) {
 if (grepl("\\.", x)) {
   nchar(sub(".*\\.", "", x))
 } else {
   0
 }
}

dat$dps_p <- sapply(dat$p_char, count_dp)
dat$dps_est <- sapply(dat$est_char, count_dp)
dat$dps_ci_lower <- sapply(dat$ci_lower_char, count_dp)
dat$dps_ci_upper <- sapply(dat$ci_upper_char, count_dp)

cat("Number of decimal places reported for estimates, CIs, and p-values:")
dat |> 
  select(dps_p, dps_est, dps_ci_lower, dps_ci_upper) |>
  pivot_longer(names_prefix = "dps_", 
               cols = dps_p:dps_ci_upper,
               names_to = "type",
               values_to = "decimals") |>
  mutate(decimals = factor(decimals, levels = 3:4)) |>  
  group_by(type, decimals, .drop = FALSE) |>
  tally() |> 
  as.data.frame()

```

We can see the majority of the 4dp values occur in eTable 5:

```{r echo = FALSE}
cat("Rows with some values to 4dps:")
dat |> 
  filter(!(dps_est==3) | !(dps_ci_upper==3)) |> 
  select(table_id, outcome, est_char, ci_lower_char, ci_upper_char, p)

```

Why were some values rounded to 4dp? It seems that these were cases where:

-   the estimate was cited directly in the text of the paper
-   4dp were necessary to remove ambiguity or confusion

For example, see this paragraph from the paper (4dp values in bold):

> Figure 3 shows the association between individual LNCS consumption and cognitive decline. There was a faster rate of decline in memory, verbal fluency, and global cognitive with higher consumption of aspartame (memory: β = −0.002, 95% CI −0.003 to **−0.0004**, verbal fluency: β = −0.001, 95% CI −0.002 to **−0.0001**, global cognition: β = −0.001, 95% CI −0.002 to **−0.0004**), saccharin (memory: β = −0.010, 95% CI −0.016 to −0.003, verbal fluency: β = −0.005, 95% CI −0.008 to −0.002, global cognition: β = −0.008, 95% CI −0.011 to −0.002), sorbitol (memory: β = −0.001, 95% CI −0.001 to **−0.0001**; verbal fluency: β = **−0.0008**, 95% CI −0.001 to **−0.0003**; global cognition: β = **−0.0006**, 95% CI −0.001 to −0.002), and xylitol (memory: β = −0.032, 95% CI −0.056 to −0.016; verbal fluency: β = −0.016, 95% CI −0.032 to −0.001; global cognition: β = −0.016, 95% CI −0.032 to −0.008) (Figure 3 and eTable 5).

There seem to be two distinct cases:

1.  If one bound of the confidence interval is very close to 0, then it is reasonable to use more decimal places to make it clear whether the interval covers 0. For example, in Row 1, the confidence interval to 3dp would be (-0.003, -0.000), which is less clear than (-0.003, -0.0004).

2.  If the confidence interval is very narrow and close to 0, then more decimal places may be needed to ensure that the estimate can be distinguished from the bounds. For example, in Row 13, estimate and CI rounded to 3dp would be -0.001 (-0.001, 0.000), which is less clear than -0.0005 (-0.001, 0.000).

However, the authors did not apply these rules consistently. For example, in Row 2, the upper bound of the confidence interval has been extended to 4dp (-0.0001) but the estimate and lower bound have not, despite being the same value to 3dp (-0.001).

It is reasonable that the authors wanted the values cited in the text to correspond to the same values in the appendix tables. However, their solution to this problem was less than ideal.

It appears that the authors generated their results tables rounded to 3dp, and then manually edited specific values to 4dp. This violates basic principles of reproducibility and introduces the risk of manual errors or typos.

Moreover, many of the values left rounded to 3dp are essentially uninterpretable. For example, in eTable 7 there are two estimates reported as $0.000 (0.000; 0.000)$ with very different p-values. A much better solution would have been to simply rescale the variables to make the coefficients larger.


![](images/4_table7.png){width="500"}

<br>


## 5. Frequent multiples of 0.008

*Note: this section has been edited in response to helpful suggestions from commentators.*

A large proportion of the estimates and CI bounds are multiples of 0.008.

In fact, in eTable 6, **all estimates and CIs** are multiples of 0.008.


![](images/3_table6.png){width="500"}


<br>

Across all 9 tables, **56%** (!) of estimates and confidence interval bounds are divisible by 0.008:

```{r include = FALSE}

cat("Proportion of values divisible by 0.008 (all):")

dat_8 |>
  summarize(prop_div = round(sum(div8)/n(), 2)) |>
  as.data.frame()

```

```{r echo = FALSE}

cat("Proportion of values divisible by 0.008 (by type):")

dat_8 |>
  group_by(name) |>
  summarize(prop_div = round(sum(div8)/n(), 2)) |>
  as.data.frame()

```

```{r, include=FALSE}

dat |>
  mutate(
    p8 = ifelse(p %% 0.008 ==0, 1, 0)
  ) |>
  summarize(p = round(sum(p8)/n(), 2)) |>
  pull(p)

```

In contrast, only 11% of p-values are divisible by 0.008. 

This is roughly in line with what we would expect under the null hypothesis where values are drawn from a uniform distribution over $[0,1]$ and rounded to 3 decimal places. (Since $1000/8 = 125$, if we drew numbers from 0 to 1,000 we would expect $126/1001 \approx 12.5\%$ to be divisible by 8.) 

The frequency of 0.008 multiples varies by table, though all tables have some:

```{r echo=FALSE}

tab_props <- dat |>
  select(table_id, est, ci_lower, ci_upper, p) |>
  pivot_longer(cols = est:p) |>
  filter(!(name == "p")) |>
  mutate(div8 = ifelse(value %% 0.008 == 0, 1, 0)) |>
  group_by(table_id) |>
  summarize(prop_div = round(sum(div8)/n(), 2)) |>
  arrange(-prop_div) |>
  as.data.frame() 


```

```{r echo = FALSE}

cat("Proportion of estimates + CIs divisible by 0.008, by table")
tab_props

```


What could be causing this unusual pattern?

We know that the median time from wave 1 to wave 3 of the survey was **8 years**. And the table titles reference "cognitive decline during eight years of follow-up". 

So perhaps they took the interaction coefficients `EV * Time` and then multiplied them by 8?

If the authors did rescale the coefficients like this, they do not mention it in the paper or appendix (as far as I can see). 

It's hard to figure this out because we are lacking some key pieces of information:

- The paper never actually gives a substantive interpretation of a coefficient, only *relative* interpretations (e.g. "Participants in the 2 highest tertiles of combined LNCS consumption  had 110% and 173% higher rates of verbal fluency decline").

- The tables only show the interaction coefficients `EV * Time` not the main coefficient on `Time` which would show the trend among the reference category. So we can't actually tell where those percentages came from.

- Although the variable is labelled as `Time` and referred to in the text as "timescale", it is actually the age of the participant at each wave. ("The timescale was the age of the participant in each wave.")

- In the tables using the tertile variable, the coefficients are labelled as interaction terms ("Tertile 1*Time") but in the tables using the individual LNCS variables we just have the name (e.g. "Aspartame") - although presumably these are also interaction coefficients.

- The coefficients are very small in magnitude and are mostly rounded to 3dp, which makes it hard to tell what the unscaled coefficients would have been. Presumably the authors must have rounded those coefficients *before* multiplying by 8 in order to produce so many values that are larger multiples in the tables (e.g. -0.168). 


## Conclusions

- There are many errors, inconsistencies, and oddities in this paper
- At least some of these issues should have been immediately obvious to the authors and to any careful reader
- The rounding inconsistency and the prevalence of sign errors suggests that the results tables generated by statistical software have been manually altered or corrupted in some way
- In at least one case, an error has caused an estimate to be presented as statistically significant when it is not
- This paper does not meet basic standards of consistency, transparency, and reproducibility

